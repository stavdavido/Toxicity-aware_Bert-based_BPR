{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abd104d-09fe-477c-baf6-3f52ebdb5b24",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797bb6bc-2310-4bb6-afb9-0eec209338b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import scispacy\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b02c37-6903-4966-9f5f-164bddde0277",
   "metadata": {},
   "source": [
    "Extract Progress Notes' Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb93705-37ff-42dc-8b1f-6cd25ad0d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading notes...\n",
      "Extracting headers...\n",
      "\n",
      "Top 50 Section Headers:\n",
      " 1. impression                               (456 occurrences)\n",
      " 2. discharge diagnoses                      (240 occurrences)\n",
      " 3. past medical history                     (179 occurrences)\n",
      " 4. discharge medications                    (168 occurrences)\n",
      " 5. reason for this examination              (126 occurrences)\n",
      " 6. htn                                      (109 occurrences)\n",
      " 7. nc                                       (95 occurrences)\n",
      " 8. leuk-neg                                 (85 occurrences)\n",
      " 9. medications                              (78 occurrences)\n",
      "10. imaging                                  (67 occurrences)\n",
      "11. admission labs                           (65 occurrences)\n",
      "12. n/a                                      (62 occurrences)\n",
      "13. medications on discharge                 (61 occurrences)\n",
      "14. discharge diagnosis                      (61 occurrences)\n",
      "15. medications on admission                 (57 occurrences)\n",
      "16. hospital course                          (57 occurrences)\n",
      "17. final diagnosis                          (51 occurrences)\n",
      "18. gerd                                     (50 occurrences)\n",
      "19. pulses                                   (46 occurrences)\n",
      "20. discharge labs                           (44 occurrences)\n",
      "21. findings                                 (43 occurrences)\n",
      "22. pmh                                      (39 occurrences)\n",
      "23. microbiology                             (38 occurrences)\n",
      "24. physical exam                            (34 occurrences)\n",
      "25. transitional issues                      (33 occurrences)\n",
      "26. mvi                                      (32 occurrences)\n",
      "27. copd                                     (30 occurrences)\n",
      "28. comments                                 (30 occurrences)\n",
      "29. micro                                    (30 occurrences)\n",
      "30. cxr                                      (29 occurrences)\n",
      "31. inpatient                                (29 occurrences)\n",
      "32. discharge instructions                   (26 occurrences)\n",
      "33. call your surgeon immediately if you experience any of the following  (26 occurrences)\n",
      "34. negative                                 (25 occurrences)\n",
      "35. na                                       (25 occurrences)\n",
      "36. studies                                  (24 occurrences)\n",
      "37. admission physical exam                  (24 occurrences)\n",
      "38. mcg/ml _________________________________________________________ (23 occurrences)\n",
      "39. discharge exam                           (23 occurrences)\n",
      "40. interpretation                           (23 occurrences)\n",
      "41. cad                                      (22 occurrences)\n",
      "42. htn                                      (21 occurrences)\n",
      "43. admission exam                           (21 occurrences)\n",
      "44. n/c                                      (20 occurrences)\n",
      "45. pe                                       (19 occurrences)\n",
      "46. pre-bypass                               (19 occurrences)\n",
      "47. ctab                                     (19 occurrences)\n",
      "48. care recommendations                     (19 occurrences)\n",
      "49. md                                       (19 occurrences)\n",
      "50. contraindications for iv contrast        (19 occurrences)\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "MIMIC_NOTES_PATH = r'C:\\Users\\Administrator\\Desktop\\medllm evn\\Data\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz'  # update to your actual path\n",
    "NOTE_CATEGORIES = ['Discharge summary', 'Physician']\n",
    "SAMPLE_SIZE = 2000  # adjust for speed vs. coverage\n",
    "\n",
    "# === 1. Load Data ===\n",
    "print(\"Loading notes...\")\n",
    "notes_df = pd.read_csv(MIMIC_NOTES_PATH, dtype={\"TEXT\": str}, usecols=[\"CATEGORY\", \"TEXT\"])\n",
    "notes_df = notes_df[notes_df[\"CATEGORY\"].isin(NOTE_CATEGORIES)].dropna(subset=[\"TEXT\"])\n",
    "notes_sample = notes_df.sample(n=min(SAMPLE_SIZE, len(notes_df)), random_state=42)[\"TEXT\"]\n",
    "\n",
    "# === 2. Extract Candidate Section Headers ===\n",
    "def extract_headers(text):\n",
    "    # Match UPPERCASE lines (possibly ending with :) as section headers\n",
    "    pattern = re.compile(r'^\\s*([A-Z][A-Z\\s\\-_/]+):?\\s*$', re.MULTILINE)\n",
    "    return pattern.findall(text)\n",
    "\n",
    "# === 3. Normalize Headers ===\n",
    "def normalize_header(header):\n",
    "    header = header.lower()\n",
    "    header = header.strip(\": \")\n",
    "    header = header.replace(\"hx\", \"history\")\n",
    "    header = re.sub(r'\\s+', ' ', header)\n",
    "    return header\n",
    "\n",
    "# === 4. Run Extraction ===\n",
    "header_counter = Counter()\n",
    "print(\"Extracting headers...\")\n",
    "for note in notes_sample:\n",
    "    raw_headers = extract_headers(note)\n",
    "    norm_headers = [normalize_header(h) for h in raw_headers]\n",
    "    header_counter.update(norm_headers)\n",
    "\n",
    "# === 5. Show Results ===\n",
    "print(\"\\nTop 50 Section Headers:\")\n",
    "for i, (header, count) in enumerate(header_counter.most_common(50), 1):\n",
    "    print(f\"{i:2d}. {header:40s} ({count} occurrences)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4eb3c2-cb9d-479a-8549-2c348598814d",
   "metadata": {},
   "source": [
    "Create User Notes Embeddings per Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad56b045-2c0c-4680-8f53-476429aad923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_sci_md' (0.5.0) was trained with spaCy v3.2.3 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\sklearn\\base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\sklearn\\base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SECTION_HEADERS = list(header_counter.keys())\n",
    "\n",
    "def split_into_sections(text):\n",
    "    # Convert to lowercase for easier matching\n",
    "    text = text.lower()\n",
    "    pattern = '|'.join([fr'\\n.*{header}.*\\n' for header in SECTION_HEADERS])\n",
    "    splits = re.split(pattern, text)\n",
    "    return [s.strip() for s in splits if s.strip()]\n",
    "\n",
    "nlp = spacy.load('en_core_sci_md')\n",
    "\n",
    "def split_into_sentences(section_texts):\n",
    "    sentences = []\n",
    "    for section in section_texts:\n",
    "        doc = nlp(section)\n",
    "        sentences.extend([sent.text.strip() for sent in doc.sents if sent.text.strip()])\n",
    "    return sentences\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model.eval()\n",
    "\n",
    "def encode_sentences(sentences, batch_size=16, max_len=128):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            all_embeddings.append(cls_embeddings)\n",
    "    return torch.cat(all_embeddings).cpu().numpy()\n",
    "\n",
    "def aggregate_patient_embedding(note_text):\n",
    "    sections = split_into_sections(note_text)\n",
    "    sentences = split_into_sentences(sections)\n",
    "    sentence_embeddings = encode_sentences(sentences)\n",
    "    patient_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "    return normalize(patient_embedding.reshape(1, -1))[0]  # Normalize per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "680f2a7c-82ab-4f5a-aabe-bf15be641327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m notes = pd.read_csv(\n\u001b[32m      2\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mAdministrator\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmedllm evn\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mData\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmimic-iii-clinical-database-1.4\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNOTEEVENTS.csv.gz\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     dtype={\u001b[32m4\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[32m5\u001b[39m: \u001b[38;5;28mstr\u001b[39m}  \u001b[38;5;66;03m# or int, float, etc. depending on data\u001b[39;00m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m notes = notes[notes[\u001b[33m\"\u001b[39m\u001b[33mCATEGORY\u001b[39m\u001b[33m\"\u001b[39m].isin([\u001b[33m\"\u001b[39m\u001b[33mDischarge summary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPhysician\u001b[39m\u001b[33m\"\u001b[39m])]\n\u001b[32m      7\u001b[39m notes = notes.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHADM_ID\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser.read(nrows)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._engine.read(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   1924\u001b[39m         nrows\n\u001b[32m   1925\u001b[39m     )\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28mself\u001b[39m._reader.read_low_memory(nrows)\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\medllm\\Lib\\gzip.py:346\u001b[39m, in \u001b[36mGzipFile.closed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mpeek() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer.peek(n)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "notes = pd.read_csv(\n",
    "    r'...\\NOTEEVENTS.csv.gz',\n",
    "    dtype={4: str, 5: str}  # or int, float, etc. depending on data\n",
    ")\n",
    "\n",
    "notes = notes[notes[\"CATEGORY\"].isin([\"Discharge summary\"])]\n",
    "notes = notes.dropna(subset=[\"TEXT\", \"HADM_ID\"])\n",
    "\n",
    "patient_texts = notes.groupby(\"HADM_ID\")[\"TEXT\"].apply(lambda x: \"\\n\".join(x)).reset_index()\n",
    "\n",
    "patient_embeddings = []\n",
    "\n",
    "for _, row in patient_texts.iterrows():\n",
    "    embedding = aggregate_patient_embedding(row[\"TEXT\"])\n",
    "    #print(embedding)\n",
    "    patient_embeddings.append(embedding)\n",
    "\n",
    "# Resulting matrix\n",
    "embeddings = np.array(patient_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690007c-a630-480a-be66-cb46eff9d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(r'...\\useremb.npz', array1=embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
